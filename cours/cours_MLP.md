# ğŸ¤– Cours : Lâ€™algorithme du Perceptron Multicouche (MLP)

## 1. Principe gÃ©nÃ©ral

Le **Perceptron Multicouche (MLP)** est un **algorithme dâ€™apprentissage supervisÃ©** appartenant Ã  la famille des **rÃ©seaux de neurones artificiels**.  
Il apprend Ã  prÃ©dire une **sortie (y)** Ã  partir dâ€™une **entrÃ©e (x)** grÃ¢ce Ã  un **ensemble de poids ajustÃ©s automatiquement** durant lâ€™entraÃ®nement.

â¡ï¸ Le but du MLP est de **trouver les poids** qui minimisent lâ€™erreur entre la **prÃ©diction du rÃ©seau** et la **valeur rÃ©elle attendue**.

---

## 2. Fonctionnement global

Lâ€™apprentissage dâ€™un MLP se dÃ©roule en **deux phases principales** :

1. **Propagation avant (forward pass)**  
   - On fait circuler les donnÃ©es dâ€™entrÃ©e Ã  travers les couches successives du rÃ©seau.  
   - Chaque neurone applique une **transformation linÃ©aire** suivie dâ€™une **fonction dâ€™activation non linÃ©aire**.  
   - On obtient une **sortie prÃ©dite** Ã  la fin du rÃ©seau.

2. **RÃ©tropropagation (backward pass)**  
   - On compare la sortie prÃ©dite Ã  la sortie rÃ©elle Ã  lâ€™aide dâ€™une **fonction de coÃ»t** (ex. : erreur quadratique moyenne ou entropie croisÃ©e).  
   - Lâ€™erreur obtenue est **rÃ©tropropagÃ©e** dans le rÃ©seau.  
   - Les **poids sont mis Ã  jour** selon la **descente de gradient** pour rÃ©duire cette erreur Ã  chaque itÃ©ration.

---

## 3. Ã‰tapes mathÃ©matiques dÃ©taillÃ©es

### ğŸ§® Ã‰tape 1 : Calcul de la propagation avant

Pour une couche \( l \) :
\[
z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}
\]
\[
a^{(l)} = f(z^{(l)})
\]

oÃ¹ :
- \( W^{(l)} \) : matrice des poids de la couche \( l \)
- \( b^{(l)} \) : biais
- \( f \) : fonction dâ€™activation (ex. : ReLU, SigmoÃ¯de)
- \( a^{(l-1)} \) : sortie de la couche prÃ©cÃ©dente

---

### ğŸ§  Ã‰tape 2 : Calcul de la fonction de coÃ»t

Pour mesurer lâ€™erreur :
\[
J = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(y_i, \hat{y}_i)
\]
avec :
- \( y_i \) : vraie Ã©tiquette  
- \( \hat{y}_i \) : prÃ©diction du modÃ¨le  
- \( \mathcal{L} \) : fonction de perte (souvent lâ€™entropie croisÃ©e pour la classification)

---

### ğŸ” Ã‰tape 3 : RÃ©tropropagation du gradient

On calcule le gradient de lâ€™erreur par rapport Ã  chaque poids grÃ¢ce Ã  la **rÃ¨gle de la chaÃ®ne** :

\[
\frac{\partial J}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T
\]

\[
\delta^{(l)} = (W^{(l+1)})^T \delta^{(l+1)} \odot f'(z^{(l)})
\]

Puis on met Ã  jour les poids :

\[
W^{(l)} := W^{(l)} - \eta \frac{\partial J}{\partial W^{(l)}}
\]
\[
b^{(l)} := b^{(l)} - \eta \frac{\partial J}{\partial b^{(l)}}
\]

oÃ¹ :
- \( \eta \) est le **taux dâ€™apprentissage** (learning rate)
- \( f'(z) \) est la dÃ©rivÃ©e de la fonction dâ€™activation

---

## 4. Fonctions dâ€™activation courantes

| Fonction | Expression | RÃ´le principal |
|-----------|-------------|----------------|
| **ReLU**  | \( f(x) = \max(0, x) \) | Brise la linÃ©aritÃ©, accÃ©lÃ¨re lâ€™apprentissage |
| **SigmoÃ¯de** | \( f(x) = \frac{1}{1+e^{-x}} \) | Donne des valeurs entre 0 et 1 (utile pour les probabilitÃ©s) |
| **Tanh** | \( f(x) = \tanh(x) \) | CentrÃ©e sur 0, utile pour couches intermÃ©diaires |
| **Softmax** | \( f_i(x) = \frac{e^{x_i}}{\sum_j e^{x_j}} \) | Produit une distribution de probabilitÃ© sur les classes |

---

## 5. Exemple simplifiÃ© dâ€™apprentissage

Prenons un rÃ©seau avec :
- EntrÃ©e : 3 neurones (valeurs normalisÃ©es)
- Couche cachÃ©e : 4 neurones ReLU
- Sortie : 2 neurones Softmax (classification binaire)

**Processus dâ€™apprentissage :**
1. Les donnÃ©es passent dans le rÃ©seau â†’ sortie prÃ©dite.
2. Lâ€™erreur est calculÃ©e avec la vraie Ã©tiquette.
3. Lâ€™erreur est rÃ©tropropagÃ©e â†’ mise Ã  jour des poids.
4. Le processus est rÃ©pÃ©tÃ© sur plusieurs **Ã©poques** jusquâ€™Ã  convergence.

---

## 6. Avantages et limites du MLP

### âœ… Avantages
- Capable dâ€™apprendre **des relations non linÃ©aires complexes**.  
- Fonctionne bien sur des donnÃ©es vectorisÃ©es (ex. : texte, signaux, donnÃ©es tabulaires).  
- Architecture flexible et facilement personnalisable.

### âš ï¸ Limites
- Exige un **temps dâ€™entraÃ®nement** parfois long.  
- NÃ©cessite **beaucoup de donnÃ©es** pour bien gÃ©nÃ©raliser.  
- Moins performant que des architectures sÃ©quentielles (RNN, LSTM, Transformers) pour le texte ou les sÃ©ries temporelles.

---

## 7. RÃ©sumÃ©

> ğŸ”¹ Le MLP est une **architecture de rÃ©seau de neurones Ã  propagation avant**.  
> ğŸ”¹ Il apprend par **descente de gradient** et **rÃ©tropropagation de lâ€™erreur**.  
> ğŸ”¹ Il repose sur un **enchaÃ®nement de transformations linÃ©aires + activations non linÃ©aires**.  
> ğŸ”¹ Il est simple, puissant et constitue souvent la **base de comprÃ©hension** avant dâ€™aborder les architectures plus avancÃ©es.
